{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzAMazBhyO7O"
   },
   "source": [
    "# AMMI Deep Natural Language Processing: Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2oBLu8PyV1V"
   },
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHed26lWcGwQ"
   },
   "source": [
    "In this tutorial we will train neural networks on the bAbI tasks using ParlAI framework.  \n",
    "This tutorial can be run both in google colab or on your computer.  \n",
    "The solutions will be added during the lab [here](https://fburl.com/ammi_dnlp_lab1).  \n",
    "\n",
    "We will cover the following:\n",
    "0. Introduction\n",
    "    - Introduction to ParlAI and installation\n",
    "    - Introduction to the bAbI tasks\n",
    "1. Exploring the data:\n",
    "    - Compute some statistics (number of examples in train, valid, test, size of examples...)\n",
    "    - Look at some examples\n",
    "2. Choose the appropriate metrics\n",
    "3. Baselines\n",
    "    - Ranom baseline\n",
    "    - Majority class baseline\n",
    "    - Information retrieval baseline\n",
    "4. More elaborate models\n",
    "   - Ranking model: Memory Network\n",
    "   - Generative model: Seq2Seq\n",
    "5. To go further\n",
    "    - Additional ideas to try if you want to dig deeper\n",
    "\n",
    "### ParlAI\n",
    "[ParlAI](https://github.com/facebookresearch/ParlAI/blob/master/README.md) (pronounced “par-lay”) is a framework for dialogue AI research, implemented in Python.\n",
    "\n",
    "Its goal is to provide researchers:\n",
    "\n",
    "* a unified framework for sharing, training and testing dialogue models\n",
    "* many popular datasets available all in one place -- from open-domain chitchat to visual question answering.\n",
    "* a wide set of reference models -- from retrieval baselines to Transformers.\n",
    "* seamless integration of Amazon Mechanical Turk for data collection and human evaluation\n",
    "* integration with Facebook Messenger to connect agents with humans in a chat interface\n",
    "\n",
    "Documentation can be found [here](http://www.parl.ai/static/docs/), some of this tutorial is inspired from the ParlAI documentation so feel free to go back and forth between the notebook and the documentation.\n",
    "\n",
    "\n",
    "### Setup the notebook\n",
    "If using google colab, make sure to use TPU runtime by going to ***Runtime > Change runtime type > Hardware accelerator: TPU > Save***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-k8BZE3HJaut"
   },
   "source": [
    "### Install ParlAI\n",
    "\n",
    "Start by installing ParlAI from github. The ParlAI folder will be located in the home directory at `~/ParlAI/`.  \n",
    "*Note: In a jupyter notebook, you can run arbitrary bash commands by prefixing them with a question mark, example: `!echo \"Hello World\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5377
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "l8V7QcnMI7Wk",
    "outputId": "dc2c78cd-b165-42e4-8f04-958244e5bc0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/root/ParlAI' already exists and is not an empty directory.\n",
      "running develop\n",
      "running egg_info\n",
      "writing parlai.egg-info/PKG-INFO\n",
      "writing dependency_links to parlai.egg-info/dependency_links.txt\n",
      "writing requirements to parlai.egg-info/requires.txt\n",
      "writing top-level names to parlai.egg-info/top_level.txt\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'parlai.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /usr/local/lib/python3.6/dist-packages/parlai.egg-link (link to .)\n",
      "parlai 0.1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /root/ParlAI\n",
      "Processing dependencies for parlai==0.1.0\n",
      "Searching for websocket-server==0.4\n",
      "Best match: websocket-server 0.4\n",
      "Processing websocket_server-0.4-py3.6.egg\n",
      "websocket-server 0.4 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/websocket_server-0.4-py3.6.egg\n",
      "Searching for websocket==0.2.1\n",
      "Best match: websocket 0.2.1\n",
      "Processing websocket-0.2.1-py3.6.egg\n",
      "websocket 0.2.1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/websocket-0.2.1-py3.6.egg\n",
      "Searching for tqdm==4.28.1\n",
      "Best match: tqdm 4.28.1\n",
      "Adding tqdm 4.28.1 to easy-install.pth file\n",
      "Installing tqdm script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for sphinx-rtd-theme==0.4.3\n",
      "Best match: sphinx-rtd-theme 0.4.3\n",
      "Processing sphinx_rtd_theme-0.4.3-py3.6.egg\n",
      "sphinx-rtd-theme 0.4.3 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/sphinx_rtd_theme-0.4.3-py3.6.egg\n",
      "Searching for Sphinx==1.8.5\n",
      "Best match: Sphinx 1.8.5\n",
      "Adding Sphinx 1.8.5 to easy-install.pth file\n",
      "Installing sphinx-apidoc script to /usr/local/bin\n",
      "Installing sphinx-autogen script to /usr/local/bin\n",
      "Installing sphinx-build script to /usr/local/bin\n",
      "Installing sphinx-quickstart script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for sh==1.12.14\n",
      "Best match: sh 1.12.14\n",
      "Processing sh-1.12.14-py3.6.egg\n",
      "sh 1.12.14 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/sh-1.12.14-py3.6.egg\n",
      "Searching for scipy==1.1.0\n",
      "Best match: scipy 1.1.0\n",
      "Adding scipy 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for scikit-learn==0.20.3\n",
      "Best match: scikit-learn 0.20.3\n",
      "Adding scikit-learn 0.20.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for requests==2.18.4\n",
      "Best match: requests 2.18.4\n",
      "Adding requests 2.18.4 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for regex==2018.1.10\n",
      "Best match: regex 2018.1.10\n",
      "Adding regex 2018.1.10 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for pyzmq==17.0.0\n",
      "Best match: pyzmq 17.0.0\n",
      "Adding pyzmq 17.0.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for pexpect==4.6.0\n",
      "Best match: pexpect 4.6.0\n",
      "Adding pexpect 4.6.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for numpy==1.14.6\n",
      "Best match: numpy 1.14.6\n",
      "Adding numpy 1.14.6 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for nltk==3.2.5\n",
      "Best match: nltk 3.2.5\n",
      "Adding nltk 3.2.5 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for joblib==0.12.5\n",
      "Best match: joblib 0.12.5\n",
      "Adding joblib 0.12.5 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for h5py==2.8.0\n",
      "Best match: h5py 2.8.0\n",
      "Adding h5py 2.8.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for GitPython==2.1.11\n",
      "Best match: GitPython 2.1.11\n",
      "Processing GitPython-2.1.11-py3.6.egg\n",
      "GitPython 2.1.11 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/GitPython-2.1.11-py3.6.egg\n",
      "Searching for flake8==3.7.7\n",
      "Best match: flake8 3.7.7\n",
      "Processing flake8-3.7.7-py3.6.egg\n",
      "flake8 3.7.7 is already the active version in easy-install.pth\n",
      "Installing flake8 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/flake8-3.7.7-py3.6.egg\n",
      "Searching for botocore==1.12.113\n",
      "Best match: botocore 1.12.113\n",
      "Adding botocore 1.12.113 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for boto3==1.9.113\n",
      "Best match: boto3 1.9.113\n",
      "Adding boto3 1.9.113 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for Pillow==4.1.1\n",
      "Best match: Pillow 4.1.1\n",
      "Adding Pillow 4.1.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for greenlet==0.4.15\n",
      "Best match: greenlet 0.4.15\n",
      "Adding greenlet 0.4.15 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for gevent==1.4.0\n",
      "Best match: gevent 1.4.0\n",
      "Adding gevent 1.4.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for setuptools==40.8.0\n",
      "Best match: setuptools 40.8.0\n",
      "Adding setuptools 40.8.0 to easy-install.pth file\n",
      "Installing easy_install script to /usr/local/bin\n",
      "Installing easy_install-3.6 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for docutils==0.14\n",
      "Best match: docutils 0.14\n",
      "Adding docutils 0.14 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for six==1.11.0\n",
      "Best match: six 1.11.0\n",
      "Adding six 1.11.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for alabaster==0.7.12\n",
      "Best match: alabaster 0.7.12\n",
      "Adding alabaster 0.7.12 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for Jinja2==2.10\n",
      "Best match: Jinja2 2.10\n",
      "Adding Jinja2 2.10 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for snowballstemmer==1.2.1\n",
      "Best match: snowballstemmer 1.2.1\n",
      "Adding snowballstemmer 1.2.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for packaging==19.0\n",
      "Best match: packaging 19.0\n",
      "Adding packaging 19.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for Pygments==2.1.3\n",
      "Best match: Pygments 2.1.3\n",
      "Adding Pygments 2.1.3 to easy-install.pth file\n",
      "Installing pygmentize script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for Babel==2.6.0\n",
      "Best match: Babel 2.6.0\n",
      "Adding Babel 2.6.0 to easy-install.pth file\n",
      "Installing pybabel script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for sphinxcontrib-websupport==1.1.0\n",
      "Best match: sphinxcontrib-websupport 1.1.0\n",
      "Adding sphinxcontrib-websupport 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for imagesize==1.1.0\n",
      "Best match: imagesize 1.1.0\n",
      "Adding imagesize 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for urllib3==1.22\n",
      "Best match: urllib3 1.22\n",
      "Adding urllib3 1.22 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for certifi==2019.3.9\n",
      "Best match: certifi 2019.3.9\n",
      "Adding certifi 2019.3.9 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for idna==2.6\n",
      "Best match: idna 2.6\n",
      "Adding idna 2.6 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for ptyprocess==0.6.0\n",
      "Best match: ptyprocess 0.6.0\n",
      "Adding ptyprocess 0.6.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for gitdb2==2.0.5\n",
      "Best match: gitdb2 2.0.5\n",
      "Processing gitdb2-2.0.5-py3.6.egg\n",
      "gitdb2 2.0.5 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/gitdb2-2.0.5-py3.6.egg\n",
      "Searching for pyflakes==2.1.1\n",
      "Best match: pyflakes 2.1.1\n",
      "Processing pyflakes-2.1.1-py3.6.egg\n",
      "pyflakes 2.1.1 is already the active version in easy-install.pth\n",
      "Installing pyflakes script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/pyflakes-2.1.1-py3.6.egg\n",
      "Searching for pycodestyle==2.5.0\n",
      "Best match: pycodestyle 2.5.0\n",
      "Processing pycodestyle-2.5.0-py3.6.egg\n",
      "pycodestyle 2.5.0 is already the active version in easy-install.pth\n",
      "Installing pycodestyle script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/pycodestyle-2.5.0-py3.6.egg\n",
      "Searching for mccabe==0.6.1\n",
      "Best match: mccabe 0.6.1\n",
      "Processing mccabe-0.6.1-py3.6.egg\n",
      "mccabe 0.6.1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/mccabe-0.6.1-py3.6.egg\n",
      "Searching for entrypoints==0.3\n",
      "Best match: entrypoints 0.3\n",
      "Adding entrypoints 0.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for python-dateutil==2.5.3\n",
      "Best match: python-dateutil 2.5.3\n",
      "Adding python-dateutil 2.5.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for jmespath==0.9.4\n",
      "Best match: jmespath 0.9.4\n",
      "Adding jmespath 0.9.4 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for s3transfer==0.2.0\n",
      "Best match: s3transfer 0.2.0\n",
      "Adding s3transfer 0.2.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for olefile==0.46\n",
      "Best match: olefile 0.46\n",
      "Adding olefile 0.46 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for pyparsing==2.3.1\n",
      "Best match: pyparsing 2.3.1\n",
      "Adding pyparsing 2.3.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for pytz==2018.9\n",
      "Best match: pytz 2018.9\n",
      "Adding pytz 2018.9 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for smmap2==2.0.5\n",
      "Best match: smmap2 2.0.5\n",
      "Processing smmap2-2.0.5-py3.6.egg\n",
      "smmap2 2.0.5 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages/smmap2-2.0.5-py3.6.egg\n",
      "Finished processing dependencies for parlai==0.1.0\n"
     ]
    }
   ],
   "source": [
    "# Remove `> /dev/null` to see the output of commands\n",
    "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
    "!cd ~/ParlAI; python setup.py develop #> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJDOC-zsL3wR"
   },
   "source": [
    "Most of the scripts that we will use in ParlAI are located in the `~/ParlAI/examples` directory.  \n",
    "Let's have a first glance at the scripts available, we will come back to them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uyboc-cm3faS"
   },
   "outputs": [],
   "source": [
    "#import parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "bIMjwxKrLm36",
    "outputId": "fcfe9460-2782-499d-9143-f8c4cf022b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_train.py\t       eval_model.py\t\t remote.py\n",
      "build_dict.py\t       extract_image_feature.py  seq2seq_train_babi.py\n",
      "build_pytorch_data.py  interactive.py\t\t train_model.py\n",
      "display_data.py        profile_train.py\n",
      "display_model.py       README.md\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ParlAI/examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4UnbCdcqHPu"
   },
   "source": [
    "### The bAbI tasks\n",
    "Many datasets and tasks are included in ParlAI, we will focus on the bAbI tasks.\n",
    "The bAbI tasks are 20 synthetic tasks that each test a unique aspect of text and reasoning, and hence test different capabilities of learning models from [Weston et al. ‘16](http://arxiv.org/abs/1502.05698).\n",
    "\n",
    "---\n",
    "**Question 0.**  \n",
    "Open the bAbI [paper](https://arxiv.org/pdf/1502.05698.pdf) and read the abstract  and section: *\"3 The Tasks\"* (until paragraph **Two or Three Supporting Facts**,  included).  \n",
    "- Explain in your own words the motivations behind these tasks (in 2-3 sentences).\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "The motivation behind these task are: produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. The main idea is to provide a set of tasks, in a similar way to how software testing is built in computer science. Each task provides a set of training and test data, with the intention that a successful model performs well on test data.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "These tasks can be downloaded and used directly from ParlAI.  \n",
    "We will focus on tasks 1, 2 and 3, see examples below:\n",
    "\n",
    "\n",
    "**Task 1: Single Supporting Fact**  \n",
    "Mary went to the bathroom.  \n",
    "John moved to the hallway.  \n",
    "Mary travelled to the office.  \n",
    "Where is Mary?  \n",
    "**Answer: office**  \n",
    "\n",
    "\n",
    "**Task 2: Two Supporting Facts**  \n",
    "John is in the playground.  \n",
    "John picked up the football.  \n",
    "Bob went to the kitchen.  \n",
    "Where is the football?  \n",
    "**Answer: playground**\n",
    "\n",
    "\n",
    "**Task 3: Three Supporting Facts **  \n",
    "John picked up the apple.  \n",
    "John went to the office.  \n",
    "John went to the kitchen.  \n",
    "John dropped the apple.   \n",
    "Where was the apple before the kitchen?  \n",
    "**Answer: office**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6IZA9r0MjBd"
   },
   "source": [
    "## 1. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Cc-Si6SyatK"
   },
   "source": [
    "First we need to download the data, we will use the `build_dict.py` as a dummy task to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "XAEH7xUXtxFj",
    "outputId": "873b3e3d-3595-433a-85c1-aab59cc7d46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading babi.tar.gz: 100% 19.2M/19.2M [00:00<00:00, 38.6MB/s]\n",
      "Building dictionary: 100% 900/900 [00:00<00:00, 19.3kex/s]\n",
      "1 Mary moved to the bathroom.\n",
      "2 John went to the hallway.\n",
      "3 Where is Mary? \tbathroom\n",
      "4 Daniel went back to the hallway.\n",
      "5 Sandra moved to the garden.\n",
      "6 Where is Daniel? \thallway\n",
      "7 John moved to the office.\n",
      "8 Sandra journeyed to the bathroom.\n",
      "9 Where is Daniel? \thallway\n",
      "10 Mary moved to the hallway.\n",
      "11 Daniel travelled to the office.\n",
      "12 Where is Daniel? \toffice\n",
      "13 John went back to the garden.\n",
      "14 John moved to the bedroom.\n",
      "15 Where is Sandra? \tbathroom\n",
      "1 Mary went to the bedroom.\n",
      "2 John journeyed to the bathroom.\n",
      "3 Where is John? \tbathroom\n",
      "4 Sandra journeyed to the hallway.\n",
      "5 John journeyed to the garden.\n",
      "6 Where is Mary? \tbedroom\n",
      "7 John journeyed to the bathroom.\n",
      "8 Sandra journeyed to the garden.\n",
      "9 Where is John? \tbathroom\n",
      "10 Sandra went back to the bedroom.\n",
      "11 Daniel travelled to the bathroom.\n",
      "12 Where is John? \tbathroom\n",
      "13 John went to the office.\n",
      "14 Mary moved to the office.\n",
      "15 Where is Sandra? \tbedroom\n"
     ]
    }
   ],
   "source": [
    "# Download the data silently\n",
    "!python ~/ParlAI/examples/build_dict.py --task babi:task1k:1 --dict-file /tmp/babi1.dict > /dev/null\n",
    "# Print a few examples\n",
    "!head -n 30 ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dZ4HiLH8IOG9"
   },
   "outputs": [],
   "source": [
    "#!head -n 30 ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa6_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbc5n2xpPnsb"
   },
   "source": [
    "The bAbI tasks were downloaded in `~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/`\n",
    "\n",
    "In bAbI the data is organised as follows:\n",
    "- **Dialog turn**: A dialog turn is a single utterance / statement. Each line in the file corresponds to one dialog turn.   \n",
    "  Example: *\"John went to the office.\"*\n",
    "- **Sample (question)**: Every few dialog turns, a question can be asked that the model has to answer, this consitute a sample.  The question is followed by its ground truth answer, separated by a tab.\n",
    "  Example: *\"Where is John? `<tab>` bathroom\"*\n",
    "- **Episode**: a sequence of ordered coherent dialog turns that are related to each other form an episode. Each new episode is independant of the others. Each line starts with the dialog turn number in the current episode.\n",
    "\n",
    "\n",
    "---\n",
    "**Question 1.**\n",
    "- **1.a.** Look at the training file of task 1 (`~/ParlAI/data/bAbI/tasks_1-20_v1-2/en/qa1_train.txt`) and compute the following information:\n",
    "  - Number of episodes\n",
    "  - Number of  samples (questions)\n",
    "  - Number of dialog turns per episode\n",
    "  - How many different answers are there in the train set? How many times does each appear? (*hint: Use a python [counter](https://docs.python.org/3/library/collections.html#collections.Counter)*)\n",
    "  - How many unique words appear in the training set? How many time does each appear? (*hint: Use the Counter `most_common()` method*)\n",
    "\n",
    "*Print the answer in the following code cell*\n",
    "  \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "nIr9Jpa6D3_R",
    "outputId": "e9daea0a-1d7e-4026-8b13-be2b6c0a835d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa10_test.txt\tqa15_train.txt\tqa1_train.txt.lengths  qa5_valid.txt\n",
      "qa10_train.txt\tqa15_valid.txt\tqa1_valid.txt\t       qa6_test.txt\n",
      "qa10_valid.txt\tqa16_test.txt\tqa20_test.txt\t       qa6_train.txt\n",
      "qa11_test.txt\tqa16_train.txt\tqa20_train.txt\t       qa6_valid.txt\n",
      "qa11_train.txt\tqa16_valid.txt\tqa20_valid.txt\t       qa7_test.txt\n",
      "qa11_valid.txt\tqa17_test.txt\tqa2_test.txt\t       qa7_train.txt\n",
      "qa12_test.txt\tqa17_train.txt\tqa2_train.txt\t       qa7_valid.txt\n",
      "qa12_train.txt\tqa17_valid.txt\tqa2_valid.txt\t       qa8_test.txt\n",
      "qa12_valid.txt\tqa18_test.txt\tqa3_test.txt\t       qa8_train.txt\n",
      "qa13_test.txt\tqa18_train.txt\tqa3_train.txt\t       qa8_valid.txt\n",
      "qa13_train.txt\tqa18_valid.txt\tqa3_valid.txt\t       qa9_test.txt\n",
      "qa13_valid.txt\tqa19_test.txt\tqa4_test.txt\t       qa9_train.txt\n",
      "qa14_test.txt\tqa19_train.txt\tqa4_train.txt\t       qa9_valid.txt\n",
      "qa14_train.txt\tqa19_valid.txt\tqa4_valid.txt\n",
      "qa14_valid.txt\tqa1_test.txt\tqa5_test.txt\n",
      "qa15_test.txt\tqa1_train.txt\tqa5_train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "nk7B40wOf8nU",
    "outputId": "e11541f6-2e3b-4b58-dc4b-b730e9a94179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 1800\n",
      "Number of questions: 9000\n",
      "Number of dialog turns per episode: 15.0\n",
      "Possible answers: Counter({'bathroom': 1564, 'hallway': 1517, 'garden': 1508, 'bedroom': 1473, 'kitchen': 1471, 'office': 1467}) (6)\n",
      "Accuracy of a random model: 0.1667\n",
      "Vocabulary size: 24\n",
      "Most common words: [('to', 18000), ('the', 18000), ('Where', 9000), ('is', 9000), ('', 9000), ('went', 7225), ('Mary', 4535), ('Sandra', 4502), ('John', 4484), ('Daniel', 4479), ('journeyed', 3620), ('travelled', 3582), ('back', 3581), ('moved', 3573), ('bathroom.', 3070), ('hallway.', 3045), ('garden.', 2982), ('kitchen.', 2981), ('office.', 2963), ('bedroom.', 2959), ('John?', 2299), ('Mary?', 2265), ('Sandra?', 2244), ('Daniel?', 2192)]\n"
     ]
    }
   ],
   "source": [
    "# FILL THIS CELL\n",
    "from collections import Counter\n",
    "task_1_train_path = '/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt'\n",
    "i = 0\n",
    "n_episodes = 0\n",
    "n_questions = 0\n",
    "n_total_dialog_turns = 0\n",
    "possible_answers = Counter()\n",
    "vocabulary = Counter()\n",
    "with open(task_1_train_path, 'r') as f:\n",
    "    for line in f:\n",
    "        # Each line starts with an integer giving the dialog turn in the current episode.\n",
    "        # Each episode contains dialog turns with questions (with the answer next to it separated by a tab)\n",
    "        line = line.strip('\\n')\n",
    "        dialog_turn = int(line.split(' ')[0])# split and take the first element\n",
    "        if dialog_turn == 1:\n",
    "            n_episodes += 1\n",
    "        # Remove the dialog turn number\n",
    "        line = ' '.join(line.split(' ')[1:])\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) > 1:\n",
    "            n_questions += 1\n",
    "            possible_answers.update([fields[1]])\n",
    "        vocabulary.update(fields[0].split(' '))\n",
    "        n_total_dialog_turns += 1\n",
    "print(f'Number of episodes: {n_episodes}')\n",
    "print(f'Number of questions: {n_questions}')\n",
    "print(f'Number of dialog turns per episode: {n_total_dialog_turns/n_episodes}')\n",
    "print(f'Possible answers: {possible_answers} ({len(possible_answers)})')\n",
    "print(f'Accuracy of a random model: {1/len(possible_answers):.4f}')\n",
    "print(f'Vocabulary size: {len(vocabulary)}')\n",
    "print(f'Most common words: {vocabulary.most_common()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNYNtDHiuVq8"
   },
   "source": [
    "\n",
    "- **2.b.** Use the appropriate script from the `~/ParlAI/examples/` to take a quick look at examples of the first bAbI task.  \n",
    "Does the number of episodes and examples fit what you computed before? (*hint: you can use the argument `--task babi:task1k:1` to select the first babi task*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1839
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "f6HlIvzGgKv2",
    "outputId": "9cc75c36-ef11-4b1e-a3fc-8b90473c9b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields: agent_reply ]\n",
      "[  max_display_len: 1000 ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train:stream ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[babi:task10k:1]: Mary moved to the bathroom.\n",
      "John went to the hallway.\n",
      "Where is Mary?\n",
      "[labels: bathroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: Daniel went back to the hallway.\n",
      "Sandra moved to the garden.\n",
      "Where is Daniel?\n",
      "[labels: hallway]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: John moved to the office.\n",
      "Sandra journeyed to the bathroom.\n",
      "Where is Daniel?\n",
      "[labels: hallway]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: Mary moved to the hallway.\n",
      "Daniel travelled to the office.\n",
      "Where is Daniel?\n",
      "[labels: office]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the garden.\n",
      "John moved to the bedroom.\n",
      "Where is Sandra?\n",
      "[labels: bathroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "John journeyed to the bathroom.\n",
      "Where is John?\n",
      "[labels: bathroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "John journeyed to the garden.\n",
      "Where is Mary?\n",
      "[labels: bedroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: John journeyed to the bathroom.\n",
      "Sandra journeyed to the garden.\n",
      "Where is John?\n",
      "[labels: bathroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bedroom.\n",
      "Daniel travelled to the bathroom.\n",
      "Where is John?\n",
      "[labels: bathroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "~~\n",
      "[babi:task10k:1]: John went to the office.\n",
      "Mary moved to the office.\n",
      "Where is Sandra?\n",
      "[labels: bedroom]\n",
      "[label_candidates: hallway|bedroom|kitchen|bathroom|garden|...and 1 more]\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[ loaded 1800 episodes with a total of 9000 examples ]\n"
     ]
    }
   ],
   "source": [
    "# FILL THIS CELL\n",
    "!python3 ~/ParlAI/examples/display_data.py --task babi:task10k:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TovRmKOA858i"
   },
   "source": [
    "## 2. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9NV_vBlPSWP"
   },
   "source": [
    "The bAbI task 1 expects single word answers among a small set of possible answers.\n",
    "\n",
    "\n",
    "---\n",
    "**Question 2**  \n",
    "- **2.a.** Which metrics do you think are appropriate for evaluating a model on this task? \n",
    "-  **2.b.**  What are their respective strengths?  \n",
    "-  **2.c.** When do they fail? (find specific examples)  \n",
    "\n",
    "\n",
    "*ANSWER HERE* \n",
    "\n",
    "- **2.a** The metric that I think are appropriate for evaluating a model on this task :  Perplexity, F1 Score, Accuracy.\n",
    "- **2.b ** Their respective strengths is the better than the other. Perplexity is very bad in this cae, F1 score can walk but Accuracy is the best.\n",
    "- **2.c**  They fail when the dataset are imbalanced.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_MZKCTW6fh9"
   },
   "source": [
    "## 3. Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSexIY71yiD6"
   },
   "source": [
    "We now have a clearer idea of the data distribution and the metrics that we can use.  \n",
    "The next step is to start solving the tasks with a simple baseline. This will allow us to compare more elaborate models agains this baseline.  \n",
    "Here are a few classical baselines:\n",
    "- **Random model**: The model answers randomly among the set of possible answers for each question\n",
    "-  **Majority class**: The model always answers with the most frequent answer in the training set (majority class)\n",
    "\n",
    "We are going to reimplement these own baselines.  \n",
    "Implementing a new model in ParlAI is detailed in the [tutorial](http://parl.ai/static/docs/seq2seq_tutorial.html) but for our simple baselines, we will only need to inherit the [Agent](https://github.com/facebookresearch/ParlAI/blob/6d246842d3f4e941dd3806f3d9fa62f607d48f59/parlai/core/agents.py#L50) class and override the `act()` method.\n",
    "\n",
    "---\n",
    "**Question 3**  \n",
    "- **3.a.** What would be the accuracy of a model that choses a random answer among the set of possible answers for each question? \n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "**3.a**.The accuracr of the model that choses a random answer among the set of possible answer for each question is:  1/n  where n is the number of differents possible value. in this case n = 6 then, the probability will be 1/6 = 0.1667 and the accurancy of that model (1/6)*100 = 16,6%.\n",
    "\n",
    "---\n",
    "*Note: the `%%writefile` magic command in jupyter writes the content of the cell to a file at the given path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9ahmFL0Rk6g1"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/ParlAI/parlai/agents/baseline/\n",
    "!touch ~/ParlAI/parlai/agents/baseline/random.py\n",
    "!touch ~/ParlAI/parlai/agents/baseline/majorityclass.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZLM7DtkkJN1"
   },
   "source": [
    "- **3.b.**  Design a baseline that answers a random word in the set of possible answer (run it multiple time to observe variance in results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Zx4emBSPjmE-",
    "outputId": "bf398348-1f6e-4c45-ff5e-d737ef8e077e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/baseline/random.py\n"
     ]
    }
   ],
   "source": [
    "# FILL THIS CELL\n",
    "%%writefile ~/ParlAI/parlai/agents/baseline/random.py\n",
    "import random\n",
    "\n",
    "from parlai.core.torch_agent import Agent\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "  \n",
    "    def act(self):\n",
    "        if 'label_candidates' not in self.observation:\n",
    "            return\n",
    "        candidates = list(self.observation['label_candidates'])\n",
    "        reply = {'text': candidates[random.randrange(len(candidates))]}\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "dMOWxgVj1Joz",
    "outputId": "e85768dc-74a5-4b70-8181-20235a333f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/random | grep accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1732
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "oVs3pOKE2-WE",
    "outputId": "a56ba5d9-6af2-4acb-ded8-42bd7db56d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: baseline/random ]\n",
      "[  model_file: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: bathroom]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: bathroom]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: bathroom]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: hallway]\n",
      "   kitchen\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: garden]\n",
      "   garden\n",
      "~~\n",
      "[babi:task10k:1]: Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   garden\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   garden\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[label_candidates: bedroom|garden|kitchen|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   bedroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/random -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UnE4oVakR4k"
   },
   "source": [
    "- **3.c.**  Design a baseline that answers the most common answer every time (majority class baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "hHIV2NUzGzQc",
    "outputId": "47ec1f44-6fde-4c04-9a33-06f931ecf9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/baseline/majorityclass.py\n"
     ]
    }
   ],
   "source": [
    "# FILL THIS CELL\n",
    "%%writefile ~/ParlAI/parlai/agents/baseline/majorityclass.py\n",
    "import random\n",
    "\n",
    "from parlai.core.torch_agent import Agent\n",
    "\n",
    "\n",
    "class MajorityclassAgent(Agent):\n",
    "  \n",
    "    def act(self):\n",
    "        # From the previous answers:\n",
    "        # Possible answers: Counter({'bathroom': 1564, 'hallway': 1517, 'garden': 1508, 'bedroom': 1473, 'kitchen': 1471, 'office': 1467}) (6)\n",
    "        # So the most common answer is 'bathroom'\n",
    "        reply = {'text': 'bathroom'}\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "U34z6bx0lJV9",
    "outputId": "a2317a57-fa34-4d39-9a70-66be42f63bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/majorityclass | grep accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1732
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "X8LHkfUS3AL0",
    "outputId": "13b2eb19-a6ea-4b79-ce23-984222f62f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: baseline/majorityclass ]\n",
      "[  model_file: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[eval_labels: hallway]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[eval_labels: garden]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: office|garden|kitchen|bathroom|hallway|...and 1 more]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/majorityclass -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOW4ZTSekWt9"
   },
   "source": [
    "---\n",
    "- **3.d.**  In which cases would the majority class baseline be better than the random baseline?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "- The majority class baseline be better than random baseline in this case if the number of times that the majority class appear in the data is  bigger than 50% of the all possible answer. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "layL9aTK_D1a"
   },
   "source": [
    "Another slightly more advanced baseline is implemented in ParlAI: the information retrieval baseline (`ir_baseline`)\n",
    "\n",
    "---\n",
    "- **3.e.** Look at the [implementation](https://github.com/facebookresearch/ParlAI/blob/53ea58acf389bffc79c85c43bcdd848eecdcecb4/parlai/agents/ir_baseline/ir_baseline.py#L211) of the IR baseline and explain in a few lines how it works (*hint: look at the following methods `act()` `rank_candidates()`  `score_match()`*)  \n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "the code plan to implement the following variants:\n",
    "Given an input message, either: find the most similar message in the (training) dataset and output the response from that exchange;  or  find the most similar response to the input directly. and  if label_candidates are provided, simply ranks them according to their similarity to the input message.\n",
    "\n",
    "The score_match() function is use to calculate the score match between the query representation the text and  return the float score of match. The function Rank_candidates to compute the score of each candidates the it store Rank candidates given representation of query. Use the function act() to Generate a response to the previously seen observation(s) by creating a list of candidates. Then if observe the label of the candidates  with the length less than zeros, we  compare the label candidates with candidate file if set. and calculate the score max and if is not max they reply  \"I don't know.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKo6zJHwksKA"
   },
   "source": [
    "- **3.f.** Use the IR baseline and compare its with one of your baselines (random and/or majority) on bAbI tasks 1, 2 and 3.  \n",
    "    (*hint: you can use `!python ... -t babi:task1-k:{i+1}` syntax to substitute the task number in a bash command from jupyter*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "EzBAEWP7k10U",
    "outputId": "f8cca30c-e8fd-420f-8f68-fd567026ceff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ Task 1 ~\n",
      "Majority class baseline:\n",
      "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n",
      "IR baseline:\n",
      "{'exs': 1000, 'accuracy': 0.465, 'f1': 0.465, 'hits@1': 0.465, 'hits@5': 0.961, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 4.65e-10}\n",
      "~ Task 2 ~\n",
      "Majority class baseline:\n",
      "{'exs': 1000, 'accuracy': 0.17, 'f1': 0.17, 'bleu': 1.7e-10}\n",
      "IR baseline:\n",
      "{'exs': 1000, 'accuracy': 0.284, 'f1': 0.284, 'hits@1': 0.284, 'hits@5': 0.9, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 2.84e-10}\n",
      "~ Task 3 ~\n",
      "Majority class baseline:\n",
      "{'exs': 1000, 'accuracy': 0.203, 'f1': 0.203, 'bleu': 2.03e-10}\n",
      "IR baseline:\n",
      "{'exs': 1000, 'accuracy': 0.132, 'f1': 0.132, 'hits@1': 0.132, 'hits@5': 0.836, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 1.32e-10}\n"
     ]
    }
   ],
   "source": [
    "# FILL THIS CELL\n",
    "for i in range(3):\n",
    "    print(f'~ Task {i+1} ~')\n",
    "    print('Majority class baseline:')\n",
    "    !python ~/ParlAI/examples/eval_model.py -t babi:task10k:{i+1} -m baseline/majorityclass | grep accuracy\n",
    "    print('IR baseline:')\n",
    "    !python ~/ParlAI/examples/eval_model.py -t babi:task10k:{i+1} -m ir_baseline | grep accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZGFSbEwJdvx"
   },
   "source": [
    "## 4. More elaborate models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wj7onBcoyoZz"
   },
   "source": [
    "We can now continue to more elaborate models and evaluate their performance in perspective to the baselines.\n",
    "We will use the `~/ParlAI/examples/train_model.py` script. Let's first get a glance at its arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1163
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "gZa5Xl5Bxxxe",
    "outputId": "2781ca13-a1fb-449b-84ad-084ffaf0fba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train_model.py [-h] [-v] [-t TASK]\n",
      "                      [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
      "                      [-nt NUMTHREADS] [-bs BATCHSIZE] [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL] [-et EVALTASK]\n",
      "                      [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME] [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
      "                      [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS] [-vp VALIDATION_PATIENCE]\n",
      "                      [-vmt VALIDATION_METRIC] [-vmm {max,min}] [-pyt PYTORCH_TEACHER_TASK] [-pytd PYTORCH_TEACHER_DATASET]\n",
      "\n",
      "Train a model\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help\n",
      "        show this help message and exit\n",
      "\n",
      "Main ParlAI Arguments:\n",
      "  -v, --show-advanced-args\n",
      "        Show hidden command line options (advanced users only) (default: False)\n",
      "  -t, --task TASK\n",
      "        ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
      "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
      "        choose from: train, train:ordered, valid, test. to stream data add \":stream\" to any option (e.g., train:stream). by\n",
      "        default: train is random with replacement, valid is ordered, test is ordered. (default: train)\n",
      "  -nt, --numthreads NUMTHREADS\n",
      "        number of threads. Used for hogwild if batchsize is 1, else for number of threads in threadpool loading, (default: 1)\n",
      "  -bs, --batchsize BATCHSIZE\n",
      "        batch size for minibatch training schemes (default: 1)\n",
      "  -dp, --datapath DATAPATH\n",
      "        path to datasets, defaults to {parlai_dir}/data (default: /root/ParlAI/data)\n",
      "\n",
      "ParlAI Model Arguments:\n",
      "  -m, --model MODEL\n",
      "        the model class name. can match parlai/agents/<model> for agents in that directory, or can provide a fully specified\n",
      "        module for `from X import Y` via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`) (default: None)\n",
      "  -mf, --model-file MODEL_FILE\n",
      "        model file name for loading and saving models (default: None)\n",
      "  -im, --init-model INIT_MODEL\n",
      "        load model weights and dict from this file (default: None)\n",
      "\n",
      "Training Loop Arguments:\n",
      "  -et, --evaltask EVALTASK\n",
      "        task to use for valid/test (defaults to the one used for training if not set) (default: None)\n",
      "  -eps, --num-epochs NUM_EPOCHS\n",
      "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
      "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
      "        Validate every n seconds. Saves model to model_file (if set) whenever best val metric is found (default: -1)\n",
      "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
      "        Saves the model to model_file.checkpoint after every n seconds (default -1, never). (default: -1)\n",
      "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
      "        Saves the model to model_file.checkpoint after every validation (default False).\n",
      "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
      "        Validate every n epochs. Saves model to model_file (if set) whenever best val metric is found (default: -1)\n",
      "  -vp, --validation-patience VALIDATION_PATIENCE\n",
      "        number of iterations of validation where result does not improve before we stop training (default: 10)\n",
      "  -vmt, --validation-metric VALIDATION_METRIC\n",
      "        key into report table for selecting best validation (default: accuracy)\n",
      "  -vmm, --validation-metric-mode {max,min}\n",
      "        how to optimize validation metric (max or min) (default: None)\n",
      "\n",
      "PytorchData Arguments:\n",
      "  -pyt, --pytorch-teacher-task PYTORCH_TEACHER_TASK\n",
      "        Use the PytorchDataTeacher for multiprocessed data loading with a standard ParlAI task, e.g. \"babi:Task1k\" (default: None)\n",
      "  -pytd, --pytorch-teacher-dataset PYTORCH_TEACHER_DATASET\n",
      "        Use the PytorchDataTeacher for multiprocessed data loading with a pytorch Dataset, e.g. \"vqa_1\" or \"flickr30k\" (default:\n",
      "        None)\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/train_model.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2apZUNj6JrS"
   },
   "source": [
    "We can train two types of models:\n",
    "- **Generative models**: The model generates an answer from its vocabulary.\n",
    "- **Ranking models**: The model is given a list of possible answers and has to choose the correct answer. This is much easier for the model since the list of possible answers is often way smaller than the size of the vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdBwoP2K36DH"
   },
   "source": [
    "### Generative model: seq2seq with attention\n",
    "\n",
    "The generative model we are going to train is a sequence to sequence model with attention based on [Sustskever et al. 2014](https://arxiv.org/abs/1409.3215) and [Bahdanau et al. 2014](https://arxiv.org/abs/1409.0473).\n",
    "      \n",
    "- **4.a.** Briefly explain how attention works in sequence to sequence neural networks.\n",
    "- **4.b.** Do you think attention is useful for the babi tasks? How would you verify it experimentally?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "- **4.a.**  Attention has been created to solve the problem of recency bias instead of using the last hidden state to predict the next Attention look at a most likely hidden states give the weights computing using the softmax combine the selected hidden state and weights to compute the context vector and after use the context vector to predict the next word in the sequence.\n",
    "- **4.b.**  Yes Attention can be usefull for babI task. We would verify it experimentally from the text input Attention can learn which words from the text are most likely to generate a word in the answer based on the question asked.\n",
    "\n",
    "---\n",
    "- **4.c.** Train a seq2seq on bAbI task 1 (10k) and compare its results to the baselines and memory network.\n",
    "   (*hint: for faster training use the following arguments `--batchsize 32 --numthreads 1 --num-epochs 5 --hiddensize 64 --embeddingsize 64 --numlayers 1 --decoder shared`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3555
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "VIpwQj4NpY5s",
    "outputId": "ef3c0a85-6bf3-4638-ddea-0be75b428f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: seq2seq ]\n",
      "[  model_file: /tmp/babi_s2s ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  dict_build_first: True ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  num_epochs: 5.0 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_comment:  ]\n",
      "[  tensorboard_log: False ]\n",
      "[  tensorboard_metrics: None ]\n",
      "[  tensorboard_tag: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Seq2Seq Arguments: ] \n",
      "[  attention: none ]\n",
      "[  attention_length: 48 ]\n",
      "[  attention_time: post ]\n",
      "[  bidirectional: False ]\n",
      "[  decoder: shared ]\n",
      "[  dropout: 0.1 ]\n",
      "[  embeddingsize: 64 ]\n",
      "[  hiddensize: 64 ]\n",
      "[  input_dropout: 0.0 ]\n",
      "[  lookuptable: unique ]\n",
      "[  numlayers: 1 ]\n",
      "[  numsoftmax: 1 ]\n",
      "[  rnn_class: lstm ]\n",
      "[ Torch Generator Agent: ] \n",
      "[  beam_block_ngram: 0 ]\n",
      "[  beam_dot_log: False ]\n",
      "[  beam_min_length: 1 ]\n",
      "[  beam_min_n_best: 3 ]\n",
      "[  beam_size: 1 ]\n",
      "[  skip_generation: False ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "[ building dictionary first... ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[ running dictionary over data.. ]\n",
      "Building dictionary:   0% 0.00/9.00k [00:00<?, ?ex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "Building dictionary:  89% 7.97k/9.00k [00:00<00:00, 20.0kex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 19.8kex/s]\n",
      "Dictionary: saving dictionary to /tmp/babi_s2s.dict\n",
      "[ dictionary built with 26 tokens in 0s ]\n",
      "[ no model with opt yet at: /tmp/babi_s2s(.opt) ]\n",
      "Dictionary: loading dictionary from /tmp/babi_s2s.dict\n",
      "[ num words =  26 ]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[ training... ]\n",
      "[ time:2.0s total_exs:800 epochs:0.09 time_left:111.0s ] {'exs': 800, 'lr': 1, 'num_updates': 25, 'loss': 42.82, 'token_acc': 0.4781, 'nll_loss': 1.713, 'ppl': 5.545}\n",
      "[ time:4.0s total_exs:1760 epochs:0.2 time_left:99.0s ] {'exs': 960, 'lr': 1, 'num_updates': 55, 'loss': 34.09, 'token_acc': 0.5875, 'nll_loss': 1.136, 'ppl': 3.115}\n",
      "[ time:6.0s total_exs:2720 epochs:0.3 time_left:95.0s ] {'exs': 960, 'lr': 1, 'num_updates': 85, 'loss': 31.81, 'token_acc': 0.587, 'nll_loss': 1.06, 'ppl': 2.887}\n",
      "[ time:8.0s total_exs:3680 epochs:0.41 time_left:92.0s ] {'exs': 960, 'lr': 1, 'num_updates': 115, 'loss': 31.45, 'token_acc': 0.5818, 'nll_loss': 1.048, 'ppl': 2.853}\n",
      "[ time:10.0s total_exs:4640 epochs:0.52 time_left:89.0s ] {'exs': 960, 'lr': 1, 'num_updates': 145, 'loss': 30.71, 'token_acc': 0.5948, 'nll_loss': 1.024, 'ppl': 2.783}\n",
      "[ time:12.0s total_exs:5600 epochs:0.62 time_left:86.0s ] {'exs': 960, 'lr': 1, 'num_updates': 175, 'loss': 30.76, 'token_acc': 0.5854, 'nll_loss': 1.025, 'ppl': 2.788}\n",
      "[ time:14.0s total_exs:6560 epochs:0.73 time_left:84.0s ] {'exs': 960, 'lr': 1, 'num_updates': 205, 'loss': 29.82, 'token_acc': 0.5906, 'nll_loss': 0.9939, 'ppl': 2.702}\n",
      "[ time:16.0s total_exs:7520 epochs:0.84 time_left:81.0s ] {'exs': 960, 'lr': 1, 'num_updates': 235, 'loss': 29.58, 'token_acc': 0.5906, 'nll_loss': 0.9861, 'ppl': 2.681}\n",
      "[ time:18.0s total_exs:8480 epochs:0.94 time_left:79.0s ] {'exs': 960, 'lr': 1, 'num_updates': 265, 'loss': 29.58, 'token_acc': 0.588, 'nll_loss': 0.9859, 'ppl': 2.68}\n",
      "[ time:20.0s total_exs:9440 epochs:1.05 time_left:77.0s ] {'exs': 960, 'lr': 1, 'num_updates': 295, 'loss': 29.69, 'token_acc': 0.5844, 'nll_loss': 0.9896, 'ppl': 2.69}\n",
      "[ time:22.0s total_exs:10400 epochs:1.16 time_left:75.0s ] {'exs': 960, 'lr': 1, 'num_updates': 325, 'loss': 29.23, 'token_acc': 0.5833, 'nll_loss': 0.9745, 'ppl': 2.65}\n",
      "[ time:24.0s total_exs:11360 epochs:1.26 time_left:72.0s ] {'exs': 960, 'lr': 1, 'num_updates': 355, 'loss': 29.03, 'token_acc': 0.5786, 'nll_loss': 0.9675, 'ppl': 2.631}\n",
      "[ time:26.0s total_exs:12320 epochs:1.37 time_left:70.0s ] {'exs': 960, 'lr': 1, 'num_updates': 385, 'loss': 29.03, 'token_acc': 0.5802, 'nll_loss': 0.9676, 'ppl': 2.632}\n",
      "[ time:28.0s total_exs:13280 epochs:1.48 time_left:68.0s ] {'exs': 960, 'lr': 1, 'num_updates': 415, 'loss': 28.82, 'token_acc': 0.599, 'nll_loss': 0.9606, 'ppl': 2.613}\n",
      "[ time:30.0s total_exs:14240 epochs:1.58 time_left:66.0s ] {'exs': 960, 'lr': 1, 'num_updates': 445, 'loss': 28.47, 'token_acc': 0.599, 'nll_loss': 0.9492, 'ppl': 2.584}\n",
      "[ time:32.0s total_exs:15200 epochs:1.69 time_left:64.0s ] {'exs': 960, 'lr': 1, 'num_updates': 475, 'loss': 28.48, 'token_acc': 0.5974, 'nll_loss': 0.9493, 'ppl': 2.584}\n",
      "[ time:34.0s total_exs:16160 epochs:1.8 time_left:62.0s ] {'exs': 960, 'lr': 1, 'num_updates': 505, 'loss': 27.97, 'token_acc': 0.6016, 'nll_loss': 0.9323, 'ppl': 2.54}\n",
      "[ time:36.0s total_exs:17120 epochs:1.9 time_left:60.0s ] {'exs': 960, 'lr': 1, 'num_updates': 535, 'loss': 27.36, 'token_acc': 0.6266, 'nll_loss': 0.912, 'ppl': 2.489}\n",
      "[ time:38.0s total_exs:18080 epochs:2.01 time_left:58.0s ] {'exs': 960, 'lr': 1, 'num_updates': 565, 'loss': 26.32, 'token_acc': 0.65, 'nll_loss': 0.8772, 'ppl': 2.404}\n",
      "[ time:40.0s total_exs:19040 epochs:2.12 time_left:56.0s ] {'exs': 960, 'lr': 1, 'num_updates': 595, 'loss': 26.37, 'token_acc': 0.6615, 'nll_loss': 0.8792, 'ppl': 2.409}\n",
      "[ time:42.0s total_exs:20000 epochs:2.22 time_left:54.0s ] {'exs': 960, 'lr': 1, 'num_updates': 625, 'loss': 25.82, 'token_acc': 0.6604, 'nll_loss': 0.8605, 'ppl': 2.364}\n",
      "[ time:44.0s total_exs:20960 epochs:2.33 time_left:52.0s ] {'exs': 960, 'lr': 1, 'num_updates': 655, 'loss': 25.95, 'token_acc': 0.6542, 'nll_loss': 0.8649, 'ppl': 2.375}\n",
      "[ time:46.0s total_exs:21920 epochs:2.44 time_left:50.0s ] {'exs': 960, 'lr': 1, 'num_updates': 685, 'loss': 25.27, 'token_acc': 0.6729, 'nll_loss': 0.8422, 'ppl': 2.322}\n",
      "[ time:48.0s total_exs:22880 epochs:2.54 time_left:47.0s ] {'exs': 960, 'lr': 1, 'num_updates': 715, 'loss': 26.0, 'token_acc': 0.6609, 'nll_loss': 0.8668, 'ppl': 2.379}\n",
      "[ time:50.0s total_exs:23840 epochs:2.65 time_left:45.0s ] {'exs': 960, 'lr': 1, 'num_updates': 745, 'loss': 24.8, 'token_acc': 0.6885, 'nll_loss': 0.8267, 'ppl': 2.286}\n",
      "[ time:52.0s total_exs:24800 epochs:2.76 time_left:43.0s ] {'exs': 960, 'lr': 1, 'num_updates': 775, 'loss': 23.98, 'token_acc': 0.7057, 'nll_loss': 0.7992, 'ppl': 2.224}\n",
      "[ time:54.0s total_exs:25760 epochs:2.86 time_left:41.0s ] {'exs': 960, 'lr': 1, 'num_updates': 805, 'loss': 23.63, 'token_acc': 0.7177, 'nll_loss': 0.7876, 'ppl': 2.198}\n",
      "[ time:56.0s total_exs:26720 epochs:2.97 time_left:39.0s ] {'exs': 960, 'lr': 1, 'num_updates': 835, 'loss': 23.63, 'token_acc': 0.7036, 'nll_loss': 0.7877, 'ppl': 2.198}\n",
      "[ time:58.0s total_exs:27680 epochs:3.08 time_left:37.0s ] {'exs': 960, 'lr': 1, 'num_updates': 865, 'loss': 22.61, 'token_acc': 0.7281, 'nll_loss': 0.7536, 'ppl': 2.125}\n",
      "[ time:60.0s total_exs:28640 epochs:3.18 time_left:35.0s ] {'exs': 960, 'lr': 1, 'num_updates': 895, 'loss': 22.69, 'token_acc': 0.725, 'nll_loss': 0.7565, 'ppl': 2.131}\n",
      "[ time:62.0s total_exs:29600 epochs:3.29 time_left:33.0s ] {'exs': 960, 'lr': 1, 'num_updates': 925, 'loss': 21.93, 'token_acc': 0.7505, 'nll_loss': 0.7312, 'ppl': 2.077}\n",
      "[ time:64.0s total_exs:30560 epochs:3.4 time_left:31.0s ] {'exs': 960, 'lr': 1, 'num_updates': 955, 'loss': 21.3, 'token_acc': 0.7552, 'nll_loss': 0.7098, 'ppl': 2.034}\n",
      "[ time:66.0s total_exs:31520 epochs:3.5 time_left:29.0s ] {'exs': 960, 'lr': 1, 'num_updates': 985, 'loss': 21.71, 'token_acc': 0.7531, 'nll_loss': 0.7236, 'ppl': 2.062}\n",
      "[ time:68.0s total_exs:32480 epochs:3.61 time_left:27.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1015, 'loss': 21.72, 'token_acc': 0.7443, 'nll_loss': 0.7239, 'ppl': 2.062}\n",
      "[ time:70.0s total_exs:33440 epochs:3.72 time_left:25.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1045, 'loss': 21.51, 'token_acc': 0.7443, 'nll_loss': 0.717, 'ppl': 2.048}\n",
      "[ time:72.0s total_exs:34400 epochs:3.82 time_left:23.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1075, 'loss': 20.9, 'token_acc': 0.7401, 'nll_loss': 0.6968, 'ppl': 2.007}\n",
      "[ time:74.0s total_exs:35360 epochs:3.93 time_left:21.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1105, 'loss': 20.48, 'token_acc': 0.751, 'nll_loss': 0.6828, 'ppl': 1.979}\n",
      "[ time:76.0s total_exs:36320 epochs:4.04 time_left:19.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1135, 'loss': 20.6, 'token_acc': 0.7432, 'nll_loss': 0.6865, 'ppl': 1.987}\n",
      "[ time:79.0s total_exs:37280 epochs:4.14 time_left:17.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1165, 'loss': 19.43, 'token_acc': 0.7604, 'nll_loss': 0.6475, 'ppl': 1.911}\n",
      "[ time:81.0s total_exs:38240 epochs:4.25 time_left:15.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1195, 'loss': 20.39, 'token_acc': 0.7578, 'nll_loss': 0.6797, 'ppl': 1.973}\n",
      "[ time:83.0s total_exs:39200 epochs:4.36 time_left:13.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1225, 'loss': 20.93, 'token_acc': 0.7521, 'nll_loss': 0.6975, 'ppl': 2.009}\n",
      "[ time:85.0s total_exs:40160 epochs:4.46 time_left:11.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1255, 'loss': 20.46, 'token_acc': 0.7505, 'nll_loss': 0.6822, 'ppl': 1.978}\n",
      "[ time:87.0s total_exs:41120 epochs:4.57 time_left:9.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1285, 'loss': 19.68, 'token_acc': 0.7495, 'nll_loss': 0.6559, 'ppl': 1.927}\n",
      "[ time:89.0s total_exs:42080 epochs:4.68 time_left:7.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1315, 'loss': 19.36, 'token_acc': 0.7547, 'nll_loss': 0.6453, 'ppl': 1.907}\n",
      "[ time:91.0s total_exs:43040 epochs:4.78 time_left:5.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1345, 'loss': 19.82, 'token_acc': 0.749, 'nll_loss': 0.6607, 'ppl': 1.936}\n",
      "[ time:93.0s total_exs:44000 epochs:4.89 time_left:3.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1375, 'loss': 18.94, 'token_acc': 0.7562, 'nll_loss': 0.6312, 'ppl': 1.88}\n",
      "[ time:95.0s total_exs:44960 epochs:5.0 time_left:1.0s ] {'exs': 960, 'lr': 1, 'num_updates': 1405, 'loss': 18.86, 'token_acc': 0.7484, 'nll_loss': 0.6288, 'ppl': 1.875}\n",
      "[ time:95.0s total_exs:45024 epochs:5.0 time_left:0s ] {'exs': 64, 'lr': 1, 'num_updates': 1407, 'loss': 1.082, 'token_acc': 0.7891, 'nll_loss': 0.5412, 'ppl': 1.718}\n",
      "[ num_epochs completed:5.0 time elapsed:95.3434271812439s ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[ running eval: valid ]\n",
      "valid:{'exs': 1000, 'accuracy': 0.534, 'f1': 0.534, 'bleu': 5.34e-10, 'lr': 1, 'num_updates': 1407, 'loss': 21.18, 'token_acc': 0.767, 'nll_loss': 0.5946, 'ppl': 1.812}\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
      "[ running eval: test ]\n",
      "test:{'exs': 1000, 'accuracy': 0.486, 'f1': 0.486, 'bleu': 4.86e-10, 'lr': 1, 'num_updates': 1407, 'loss': 22.39, 'token_acc': 0.743, 'nll_loss': 0.6434, 'ppl': 1.903}\n"
     ]
    }
   ],
   "source": [
    "# FILL THIS CELL\n",
    "!python ~/ParlAI/examples/train_model.py --task babi:task10k:1 --model seq2seq  --model-file /tmp/babi_s2s --batchsize 32 --numthreads 1 --num-epochs 5 --hiddensize 64 --embeddingsize 64 --numlayers 1 --decoder shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4627
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "OvxjgtEHTXFb",
    "outputId": "34527eca-69ee-473a-a05d-971ed6e5d5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: seq2seq ]\n",
      "[  model_file: /tmp/babi_s2s ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Seq2Seq Arguments: ] \n",
      "[  attention: none ]\n",
      "[  attention_length: 48 ]\n",
      "[  attention_time: post ]\n",
      "[  bidirectional: False ]\n",
      "[  decoder: same ]\n",
      "[  dropout: 0.1 ]\n",
      "[  embeddingsize: 128 ]\n",
      "[  hiddensize: 128 ]\n",
      "[  input_dropout: 0.0 ]\n",
      "[  lookuptable: unique ]\n",
      "[  numlayers: 2 ]\n",
      "[  numsoftmax: 1 ]\n",
      "[  rnn_class: lstm ]\n",
      "[ Torch Generator Agent: ] \n",
      "[  beam_block_ngram: 0 ]\n",
      "[  beam_dot_log: False ]\n",
      "[  beam_min_length: 1 ]\n",
      "[  beam_min_n_best: 3 ]\n",
      "[  beam_size: 1 ]\n",
      "[  skip_generation: False ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "Dictionary: loading dictionary from /tmp/babi_s2s.dict\n",
      "[ num words =  26 ]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[ Loading existing model params from /tmp/babi_s2s ]\n",
      "/root/ParlAI/parlai/core/torch_agent.py:730: UserWarning: LR scheduler is different from saved. Starting fresh!\n",
      "  warn_once(\"LR scheduler is different from saved. Starting fresh!\")\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[eval_labels_choice]: bathroom\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "~~\n",
      "[eval_labels_choice]: bathroom\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: hallway\n",
      "~~\n",
      "[eval_labels_choice]: bathroom\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "~~\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: hallway\n",
      "~~\n",
      "[eval_labels_choice]: hallway\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "office\n",
      "John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[eval_labels: hallway]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[eval_labels_choice]: garden\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[eval_labels: garden]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: garden\n",
      "~~\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "~~\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "~~\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "~~\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "office\n",
      "Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: kitchen|bathroom|hallway|bedroom|garden|...and 1 more]\n",
      "   [Seq2Seq]: office\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py --task babi:task10k:1 --model seq2seq --model-file /tmp/babi_s2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaX_GSktxrz1"
   },
   "source": [
    "### Ranking model: memory network\n",
    "\n",
    "We saw in the class that Memory Networks ([Sukhbaatar et al. 15'](https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)) rely on an explicit memory \"database\". this is especially adapted to tasks where a few useful memories are \"hidden\" among distractor memories.  \n",
    "These type of networks worktherefore  especially well for the bAbI tasks by turning the previous dialog turns as memories and the question as the query.  \n",
    "Here is an illustration of a single Hop in a memory network from the   Weston et al. '14:'\n",
    "\n",
    "![Memory Network schema](https://raw.githubusercontent.com/louismartin/ammi-2019-bordes-DeepNLP/master/lab1/memory_network.png)\n",
    "\n",
    "\n",
    "---\n",
    "**Question 4**  \n",
    "- **4.d.** Explain how hops work in a memory network (either with words or formulas using the notations of the above figure)\n",
    "- **4.e.** How can a memory network be used to rank multiple candidates?  \n",
    "  (*hint: you can look at the [implementation](https://github.com/facebookresearch/ParlAI/blob/6bd0e58692b3fd3a13b5f654944525ac1b7cd8e3/parlai/agents/memnn/modules.py#L22) of the memory network in ParlAI and especially the `_score()` method. Recall how the IR baseline worked.*)\n",
    "  \n",
    "*ANSWER HERE*\n",
    "  \n",
    "\n",
    " - **4.d** Input memory representation : suppose we have an input set to be stored in the memory. The set of the input are converted into memory vector by embedding each entire in continuos space. The query is also embeddeing to obtain an internal state. I n the embedding space we comupte the probability vector over the inputs.\n",
    " A hops is a memory  layer. When the task becomes more difficult we need multiple supporting memory that can include more than one set of input/output memories by stacking a number of memory layers (hops). \n",
    " The $( k+1)^{th}$ hop takes as input the output of the $k^{th}$ hop: $q_{i+1}= 0_{i}+q_{i}$. Thus the model reconsidering its answer over and over until it get to a good enough answer. Then we compute final answer argmax only on the last hop.\n",
    " \n",
    " - **4.e** The memory networks can be use multiple candidates by computing the inner product between the weighted output and the embedding candidates vector, after we take the one which the index corresponding to the argmax pf the inner product result.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHICORdji1Cx"
   },
   "source": [
    "\n",
    "- **4.f.** Using the ParlAI implementation, train a memory network on bAbI tasks 1, 2 and 3 (10k) and compare its results with the baselines.  \n",
    "   (*hint: use a 1 thread, a batch size of 32 and 5 epochs*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "iKeyD3UYsaFs",
    "outputId": "78dc7343-9fbb-480f-bfc2-d3ff6800e36f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ Task 1 ~\n",
      "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 19.8kex/s]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:392: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:221: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:392: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "valid:{'exs': 1000, 'accuracy': 0.986, 'f1': 0.986, 'hits@1': 0.986, 'hits@5': 1.0, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 9.86e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 77.29, 'mean_loss': 0.07729, 'mean_rank': 1.015}\n",
      "test:{'exs': 1000, 'accuracy': 0.981, 'f1': 0.981, 'hits@1': 0.981, 'hits@5': 1.0, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 9.81e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 74.78, 'mean_loss': 0.07478, 'mean_rank': 1.02}\n",
      "~ Task 2 ~\n",
      "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 14.3kex/s]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:392: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:221: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:392: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "valid:{'exs': 1000, 'accuracy': 0.211, 'f1': 0.211, 'hits@1': 0.211, 'hits@5': 0.867, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 2.11e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 2391.0, 'mean_loss': 2.391, 'mean_rank': 3.239}\n",
      "test:{'exs': 1000, 'accuracy': 0.219, 'f1': 0.219, 'hits@1': 0.219, 'hits@5': 0.853, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 2.19e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 2367.0, 'mean_loss': 2.367, 'mean_rank': 3.253}\n",
      "~ Task 3 ~\n",
      "Building dictionary: 100% 9.00k/9.00k [00:01<00:00, 6.57kex/s]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:392: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:221: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:392: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "valid:{'exs': 1000, 'accuracy': 0.341, 'f1': 0.341, 'hits@1': 0.341, 'hits@5': 0.976, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 3.41e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 1702.0, 'mean_loss': 1.702, 'mean_rank': 2.563}\n",
      "test:{'exs': 1000, 'accuracy': 0.403, 'f1': 0.403, 'hits@1': 0.403, 'hits@5': 0.971, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 4.03e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 1620.0, 'mean_loss': 1.62, 'mean_rank': 2.38}\n"
     ]
    }
   ],
   "source": [
    "# FILL CELL\n",
    "for i in range(3):\n",
    "    print(f'~ Task {i+1} ~')\n",
    "    !python ~/ParlAI/examples/train_model.py -t babi:task10k:{i+1} -m memnn -mf /tmp/babi{i+1}_memnn -bs 32 -eps 5 | grep \"'accuracy':\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHH0Wy34o3_W"
   },
   "source": [
    "## 5. To go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaNGupqLssUB"
   },
   "source": [
    "If you want to go further you can try to do the following:\n",
    "\n",
    "- Retrieve and plot the attention of the memory network for the different hops along the memories.\n",
    "- For the seq2seq model, can you plot the training loss? The validation loss? Both on the same plot?\n",
    "- Can you show an example of overfitting?\n",
    "- Adapt the seq2seq model for ranking using the [torch ranker tutorial](http://www.parl.ai/static/docs/tutorial_torch_ranker_agent.html)\n",
    "- Try multitasking babi and squad, does it improve the performance? (this will require more GPU power than what is available in google colab)\n",
    "- You can play around with other models and other tasks\n",
    "- Try interfacing ParlAI with [messenger](http://www.parl.ai/static/docs/tutorial_messenger.html )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "N4UnbCdcqHPu"
   ],
   "name": "ammi_dnlp_lab1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
